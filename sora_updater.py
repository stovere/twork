# import pymysql
# pymysql.install_as_MySQLdb()  # è®© peewee ä»¥ä¸ºæœ‰ MySQLdb/mysqlclient

import os
import re
import json
import jieba
from opencc import OpenCC
from dotenv import load_dotenv

if not os.getenv('GITHUB_ACTIONS'):
    load_dotenv(dotenv_path='.sora.env')
from peewee import *
from model.mysql_models import (
    DB_MYSQL, Product,Video, Document, SoraContent,  SoraMedia, FileTag, Tag, init_mysql
)
from database import ensure_connection
from model.scrap import Scrap
from utils.string_utils import LZString

SYNC_TO_POSTGRES = os.getenv('SYNC_TO_POSTGRES', 'false').lower() == 'true'
BATCH_LIMIT = None
# åˆå§‹åŒ– MySQLï¼ˆå¿…é¡»å…ˆæ‰§è¡Œï¼‰
init_mysql()

# å¦‚éœ€ PostgreSQLï¼Œå†å¯¼å…¥å¹¶åˆå§‹åŒ–
if SYNC_TO_POSTGRES:
    from model.pg_models import DB_PG, SoraContentPg, SoraMediaPg, ProductPg, init_postgres
    from playhouse.shortcuts import model_to_dict
    init_postgres()
    # try:
    #     DB_PG.connect()
    #     print("âœ”ï¸ DSN æ–¹å¼ä¸‹ï¼Œconnect() æˆåŠŸï¼")
    #     cursor = DB_PG.execute_sql("SELECT 1;")
    #     print("[test query] SELECT 1 è¿”å›ï¼š", cursor.fetchone()[0])
    # except Exception as e:
    #     print("âŒ DSN æ–¹å¼ä¸‹ï¼Œconnect() å¤±è´¥ï¼š", e)
    # finally:
    #     if not DB_PG.is_closed():
    #         DB_PG.close()
    #         print("ğŸ”’ è¿æ¥å·²å…³é—­")

# åŒä¹‰è¯å­—å…¸
SYNONYM = {
    "æ»‘é¼ ": "é¼ æ ‡",
    "è¤å¹•": "æ˜¾ç¤ºå™¨",
    "ç¬”ç”µ": "ç¬”è®°æœ¬",
}

def clean_bj_text(original_string):
    target_strings = ["ğŸ’¾"]
    for target in target_strings:
        pos = original_string.find(target)
        if pos != -1:
            original_string = original_string[:pos]
    return original_string

def clean_text(original_string):
    target_strings = ["- Advertisement - No Guarantee", "- å¹¿å‘Š - æ— æ‹…ä¿"]
    for target in target_strings:
        pos = original_string.find(target)
        if pos != -1:
            original_string = original_string[:pos]

    replace_texts = [
        "æ±‚æ‰“èµ", "æ±‚èµ", "å¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–æˆ–åˆ†äº«æ–‡ä»¶",
        "ç§èŠæ¨¡å¼ï¼šå°†å«æœ‰File IDçš„æ–‡æœ¬ç›´æ¥å‘é€ç»™æœºå™¨äºº @datapanbot å³å¯è¿›è¡Œæ–‡ä»¶è§£æ",
        "â‘ ç§èŠæ¨¡å¼ï¼šå°†å«æœ‰File IDçš„æ–‡æœ¬ç›´æ¥å‘é€ç»™æœºå™¨äºº  å³å¯è¿›è¡Œæ–‡ä»¶è§£æ",
        "å•æœºå¤åˆ¶ï¼š", "æ–‡ä»¶è§£ç å™¨:", "æ‚¨çš„æ–‡ä»¶ç å·²ç”Ÿæˆï¼Œç‚¹å‡»å¤åˆ¶ï¼š",
        "æ‰¹é‡å‘é€çš„åª’ä½“ä»£ç å¦‚ä¸‹:", "æ­¤æ¡åª’ä½“åˆ†äº«link:",
        "å¥³ä¾…æœç´¢ï¼š@ seefilebot", "è§£ç ï¼š@ MediaBK2bot",
        "å¦‚æœæ‚¨åªæ˜¯æƒ³å¤‡ä»½ï¼Œå‘é€ /settings å¯ä»¥è®¾ç½®å…³é—­æ­¤æ¡å›å¤æ¶ˆæ¯",
        "åª’ä½“åŒ…å·²åˆ›å»ºï¼", "æ­¤åª’ä½“ä»£ç ä¸º:", "æ–‡ä»¶åç§°:", "åˆ†äº«é“¾æ¥:", "|_SendToBeach_|",
        "Forbidden: bot was kicked from the supergroup chat",
        "Bad Request: chat_id is empty"
    ]
    for text in replace_texts:
        original_string = original_string.replace(text, '')

    original_string = re.sub(r"åˆ†äº«è‡³\d{4}-\d{2}-\d{2} \d{2}:\d{2} åˆ°æœŸåæ‚¨ä»å¯é‡æ–°åˆ†äº«", '', original_string)

    json_pattern = r'\{[^{}]*?"text"\s*:\s*"[^"]+"[^{}]*?\}'
    matches = re.findall(json_pattern, original_string)
    for match in matches:
        try:
            data = json.loads(match)
            if 'content' in data and isinstance(data['content'], str):
                original_string += f"\n{data['content']}"
        except json.JSONDecodeError:
            pass
        original_string = original_string.replace(match, '')

    wp_patterns = [r'https://t\.me/[^\s]+']
    for pattern in wp_patterns:
        original_string = re.sub(pattern, '', original_string)

    for pat in [
        r'LINK\s*\n[^\n]+#C\d+\s*\nOriginal:[^\n]*\n?',
        r'LINK\s*\n[^\n]+#C\d+\s*\nForwarded from:[^\n]*\n?',
        r'LINK\s*\n[^\n]*#C\d+\s*',
        r'Original caption:[^\n]*\n?'
    ]:
        original_string = re.sub(pat, '', original_string)

    original_string = re.sub(r'^\s*$', '', original_string, flags=re.MULTILINE)
    lines = original_string.split('\n')
    unique_lines = list(dict.fromkeys(lines))
    result_string = "\n".join(lines)

    for symbol in ['ğŸ”‘', 'ğŸ’']:
        result_string = result_string.replace(symbol, '\r\n' + symbol)

    return result_string[:1500] if len(result_string) > 1500 else result_string

def replace_synonym(text):
    for k, v in SYNONYM.items():
        text = text.replace(k, v)
    return text

def segment_text(text):
    text = replace_synonym(text)
    return " ".join(jieba.cut(text))

def fetch_tag_cn_for_file(file_unique_id):
    query = (
        Tag
        .select()
        .join(FileTag, on=(FileTag.tag == Tag.tag))
        .where(FileTag.file_unique_id == file_unique_id)
    )
    return [t.tag_cn for t in query if t.tag_cn]


def sync_to_postgres(record):
    if not SYNC_TO_POSTGRES:
        return

    from playhouse.shortcuts import model_to_dict

    IGNORED_FIELDS = {'content_seg_tsv', 'created_at', 'updated_at'}

    model_data = model_to_dict(record, recurse=False)
    model_data = {k: v for k, v in model_data.items() if k not in IGNORED_FIELDS}
    model_data["id"] = record.id  # æ˜¾å¼ä¸»é”®

    with DB_PG.atomic():
        try:
            existing = SoraContentPg.get(SoraContentPg.id == record.id)
            for k, v in model_data.items():
                setattr(existing, k, v)
            existing.save()
        except SoraContentPg.DoesNotExist:
            SoraContentPg.create(**model_data)

def sync_media_to_postgres(content_id, media_rows):
    if not SYNC_TO_POSTGRES:
        return

    with DB_PG.atomic():
        for media in media_rows:
            insert_data = {
                "content_id": content_id,
                "source_bot_name": media["source_bot_name"],
                "file_id": media["file_id"],
                "thumb_file_id": media["thumb_file_id"]
            }
            # print(f"Syncing media to PostgreSQL: {insert_data}")

            try:
                # print(f"ğŸ›°ï¸ Syncing media to PostgreSQL: {insert_data}")

                SoraMediaPg.insert(**insert_data).on_conflict(
                    conflict_target=[SoraMediaPg.content_id, SoraMediaPg.source_bot_name],
                    update={k: insert_data[k] for k in ['file_id', 'thumb_file_id']}
                ).execute()

            except Exception as e:
                print(f"âŒ æ’å…¥ PostgreSQL sora_media å¤±è´¥: {e}")
                # print(f"   â¤ å¤±è´¥å†…å®¹: {insert_data}")

def process_documents():
    # DB_MYSQL.connect()
    ensure_connection()  # âœ… æ¨èå†™æ³•


    print("\nğŸš€ å¼€å§‹åŒæ­¥ stage != 'updated' çš„ document åˆ° PostgreSQL...",flush=True)
    for doc in Document.select().where((Document.kc_status.is_null(True)) | (Document.kc_status != 'updated')).limit(BATCH_LIMIT):
        if not doc.file_name and not doc.caption:
            doc.kc_status = 'updated'
            doc.save()
            continue

        jieba.load_userdict("jieba_userdict.txt")
        # æ–‡æœ¬æ¸…æ´—ä¸åˆ†è¯
        file_name = LZString.extract_meaningful_name(doc.file_name or '') or ''
        content = LZString.clean_text(f"{file_name}\n{doc.caption or ''}")
        content_seg = segment_text(content)

        # æ ‡ç­¾åˆ†è¯è¿½åŠ 
        tag_seg = ''
        tag_cn_list = fetch_tag_cn_for_file(doc.file_unique_id)
        if tag_cn_list:
            tag_seg = ' '.join(f'#{tag}' for tag in tag_cn_list)
            content_seg += " " + " ".join(tag_cn_list)


        # print(f"Processing {doc.file_unique_id}",flush=True)

        tw2s = OpenCC('tw2s')
        content_seg = tw2s.convert(content_seg)

        # ç»Ÿä¸€è®°å½•æ•°æ®
        record_data = {
            'source_id': doc.file_unique_id,
            'file_type': 'd',
            'tag': tag_seg,
            'content': content,
            'content_seg': content_seg,
            'file_size': doc.file_size
        }

        # ä½¿ç”¨ get_or_create ä¿è¯å”¯ä¸€æ€§ï¼Œé¿å… Duplicate
        kw, created = SoraContent.get_or_create(source_id=doc.file_unique_id, defaults=record_data)

        if not created:
            # å·²å­˜åœ¨ï¼Œæ›´æ–°å­—æ®µ
            for key, value in record_data.items():
                setattr(kw, key, value)
            kw.save()

        # æ›´æ–° Document è®°å½•
        doc.kc_id = kw.id
        doc.kc_status = 'updated'
        doc.save()

        # åŒæ­¥ PostgreSQL
        if SYNC_TO_POSTGRES and kw.id:
            sync_to_postgres(kw)

    DB_MYSQL.close()



def process_videos():
    ensure_connection()  # âœ… æ¨èå†™æ³•
    # DB_MYSQL.connect()


    print("\nğŸš€ å¼€å§‹åŒæ­¥ stage != 'updated' çš„ video åˆ° PostgreSQL...")
    for doc in Video.select().where((Video.kc_status.is_null(True)) | (Video.kc_status != 'updated')).limit(BATCH_LIMIT):
        if not doc.file_name and not doc.caption:
            doc.kc_status = 'updated'
            doc.save()
            continue

        tag_seg = ''
        file_name = LZString.extract_meaningful_name(doc.file_name or '') or ''
        content = LZString.clean_text(f"{file_name or ''}\n{doc.caption or ''}")
        content_seg = segment_text(content)
        tag_cn_list = fetch_tag_cn_for_file(doc.file_unique_id)
        if tag_cn_list:
            tag_seg = ' '.join(f'#{tag}' for tag in tag_cn_list)
            content_seg += " " + " ".join(tag_cn_list)

        print(f"Processing {doc.file_unique_id}",flush=True)

        tw2s = OpenCC('tw2s')
        content_seg = tw2s.convert(content_seg)

        record_data = {
            'source_id': doc.file_unique_id,
            'file_type': 'v',
            'content': content,
            'tag': tag_seg,
            'content_seg': content_seg,
            'file_size': doc.file_size,
            'duration': doc.duration
        }

        kw, created = SoraContent.get_or_create(source_id=doc.file_unique_id, defaults=record_data)

        if not created:
            for key, value in record_data.items():
                setattr(kw, key, value)
            kw.save()

        # print(f"  ğŸ”„ æ›´æ–° MySQL sora_content [{kw}]",flush=True)

        # print(kw.__data__)

        doc.kc_id = kw.id
        doc.kc_status = 'updated'
        doc.save()

        if SYNC_TO_POSTGRES and kw.id:
            sync_to_postgres(kw)

    DB_MYSQL.close()



def parse_bj_tag_for_file(tag_str):
    tag_cn_list = []
    if tag_str:
       # å°† tag_str æŒ‰ç©ºç™½ç¬¦å·åˆ†å‰²
        tag_list = tag_str.split()
        for tag in tag_list:
            # ç§»é™¤ tag å‰çš„ #
            tag = tag.lstrip('#')
            
            tag_mapping = {
                "ç™½ç§äºº": "ç™½äºº",
                "é»‘ç§äºº": "é»‘äºº",
                "éœ²è„¸": "æœ‰éœ²è„¸",
                "é®æŒ¡": "å¸¦äº†é¢ç½©",
                "æœªéœ²è„¸": "æ²¡æœ‰éœ²è„¸",
                "æ— æ¯›": "é«˜å¹´çº§_å°äº”",
                "ä¸­æ¯›": "å°‘å¹´_é«˜ä¸­",
                "é»‘æ£®æ—": "å°‘å¹´_é«˜ä¸­",
                "å¹¼å„¿": "ä½å¹´çº§_å°äºŒ",
                "å°å­¦": "ä½å¹´çº§_å°äºŒ",
                "åˆä¸­": "åˆæ¯›",
                "é«˜ä¸­": "å°‘å¹´_é«˜ä¸­",
                "åˆæ³•": "å°‘å¹´_é«˜ä¸­",
                "æ¸…æ°´": "æ²¡æœ‰è£¸ä½“",
                "å†™çœŸ": "æ­£å¤ªä¸»é¢˜æ±‡æ•´",
                "åŠ¨ç”»": "å¡é€šåŠ¨æ¼«",
                "æ¯å­": "æ­£å¤ªä¸é˜¿å§¨",
                "è‚›äº¤": "çˆ†èŠ",
                "å£äº¤": "å£äº¤",
                "è¶³äº¤": "æ‹è¶³",
                "è‡ªæ’¸": "æ’¸ç®¡",
                "å°„ç²¾": "å°„ç²¾",
                "å±•ç¤º": "æ­£å¤ªç‹¬ç§€",
                "æ‘¸": "æ‰‹äº¤",
                "å·æ‹": "å·æ‹",
                "èƒ–å¤ª": "èƒ–å¤ª",
                "TKæŒ ç—’": "ç˜™ç—’",
                "SPæ‰“å±è‚¡": "æ‰“å±è‚¡",
                "æ‹ç‰©": "æ‹ç‰©",
                "çŒå¥‡é‡å£": "çŒå¥‡",
                "åŒ»å­¦ç±»": "åŒ»å­¦",
                "éœ¸å‡Œ": "éœ¸å‡Œ",
                "ç›‘è§†": "ç›‘è§†å™¨",
                "ç›´æ’­å½•å±": "ç›´æ’­",
                "å†°æ·‡æ·‹": "å†°æ·‡æ·‹",
                "å¥¶é»„åŒ…": "å¥¶é»„åŒ…",
                "çœ¼é•œå“¥": "çœ¼é•œå“¥ç³»åˆ—",
                "è¥¿è¾¹çš„é£":"è¥¿è¾¹çš„é£",
                "æ—¥æœ¬å…¨æ–¹ä½":"æ—¥æœ¬å…¨æ–¹ä½",
                "å°å­©ä¸ç¬¨":"å°å­©ä¸ç¬¨",  #
                "ç½‘è°ƒå¤§ç¥":"ç½‘è°ƒå¤§ç¥",  #
                "çŒ«ç³»åˆ—":"çŒ«ç³»åˆ—",  
                "é›¨èŠ±çŸ³":"é›¨èŠ±çŸ³ç³»åˆ—",  
                "æ—©ç‚¹ç¡è§‰caodidi":"Caodidiç³»åˆ—",
                "BBåµ¬":"BBåµ¬",
                "weå‡ºå“":"WEç³»åˆ—",
                "å ‚å±±å¤©è‰":"å ‚å±±",
                "ä¹ˆä¹ˆå“’è§†é¢‘":"ä¹ˆä¹ˆå“’è§†é¢‘", #
                "ä¹ˆä¹ˆå“’ä¸¾ç‰ŒåŸåˆ›":"ä¹ˆä¹ˆå“’ä¸¾ç‰ŒåŸåˆ›", #
                "å°6ç«¥æ¨¡":"å°å…­æ‘„å½±",
                "lediäºŒç»´ç ":"äºŒç»´ç ç³»åˆ—"
            }

            # æ›¿æ¢æ ‡ç­¾
            tag = tag_mapping.get(tag, tag)  # å¦‚æœ tag ä¸åœ¨æ˜ å°„ä¸­ï¼Œå°±ä¿ç•™åŸå€¼
            tag_cn_list.append(tag)

    return tag_cn_list

def process_scrap():
    # DB_MYSQL.connect()
    ensure_connection()  # âœ… æ¨èå†™æ³•


    print("\nğŸš€ å¼€å§‹åŒæ­¥ stage != 'updated' çš„ scrap åˆ° PostgreSQL...")
    for scrap in Scrap.select().where(((Scrap.kc_status.is_null(True)) | (Scrap.kc_status != 'updated')) & (Scrap.thumb_file_unique_id != '')).limit(BATCH_LIMIT):
        if not scrap.content:
            scrap.kc_status = 'updated'
            scrap.save()
            continue

        content = clean_bj_text(scrap.content or '')
        content = LZString.clean_text(content)
        content_seg = segment_text(content)

        tag_seg = ''
        if scrap.tag:
            tag_cn_list = parse_bj_tag_for_file(scrap.tag)
            tag_seg = ' '.join(f'#{tag}' for tag in tag_cn_list)
            content_seg += " " + " ".join(tag_cn_list)

        # print(f"Processing {scrap.id}: {content_seg}")

        tw2s = OpenCC('tw2s')
        content_seg = tw2s.convert(content_seg)

        record_data = {
            'source_id': scrap.id,
            'file_type': 's',
            'content': content,
            'content_seg': content_seg,
            'tag': tag_seg,
            'file_size': scrap.estimated_file_size,
            'duration': scrap.duration,
            'thumb_file_unique_id': scrap.thumb_file_unique_id,
            'thumb_hash': scrap.thumb_hash
        }

        kw, created = SoraContent.get_or_create(source_id=scrap.id, defaults=record_data)

        if not created:
            for key, value in record_data.items():
                setattr(kw, key, value)
            kw.save()

        scrap.kc_id = kw.id
        scrap.kc_status = 'updated'
        scrap.save()

        media_data = [{
            'source_bot_name': scrap.thumb_bot,
            'file_id': None,
            'thumb_file_id': scrap.thumb_file_id
        }]

        for media in media_data:
            existing = SoraMedia.select().where(
                (SoraMedia.content_id == scrap.kc_id) &
                (SoraMedia.source_bot_name == media["source_bot_name"])
            ).first()

            if existing:
                existing.file_id = media["file_id"]
                existing.thumb_file_id = media["thumb_file_id"]
                existing.save()
                print(f"  ğŸ”„ æ›´æ–° MySQL sora_media [{media['source_bot_name']}]")
            else:
                SoraMedia.create(content_id=scrap.kc_id, **media)
                print(f"  âœ… æ–°å¢ MySQL sora_media [{media['source_bot_name']}]")

        if SYNC_TO_POSTGRES and kw.id:
            sync_to_postgres(kw)
            sync_media_to_postgres(scrap.kc_id, media_data)
            print("ğŸš€ åŒæ­¥åˆ° PostgreSQL å®Œæˆ")

    DB_MYSQL.close()


def process_sora_update():
    import time
    ensure_connection()  # âœ… æ¨èå†™æ³•
    # DB_MYSQL.connect()


    sora_content_rows = SoraContent.select().where(SoraContent.stage=="pending").limit(BATCH_LIMIT)
    print(f"ğŸ“¦ æ­£åœ¨å¤„ç† {len(sora_content_rows)} ç¬” sora æ•°æ®...\n",flush=True)

    for row in sora_content_rows:
        source_id = row.source_id
        print(f"ğŸ” å¤„ç† source_id: {source_id}",flush=True)

        content = {
            'source_id': source_id,
            'content': row.content or '',
            'owner_user_id': row.user_id,
            'source_channel_message_id': row.source_channel_message_id,
            'thumb_file_unique_id': row.thumb_file_unique_id,
            'thumb_hash': row.thumb_hash,
            'file_size': row.file_size,
            'duration': row.duration,
            'tag': row.tag,
            'file_type': row.file_type[0] if row.file_type else None,
            'valid_state': row.valid_state,
            'plan_update_timestamp': row.plan_update_timestamp,
            'stage': row.stage
        }

        # æ’å…¥æˆ–æ›´æ–° SoraContent
        sora_content, created = SoraContent.get_or_create(source_id=source_id, defaults=content)
        if created:
            print("âœ… æ–°å¢ MySQL sora_content",flush=True)
        else:
            for k, v in content.items():
                setattr(sora_content, k, v)
            sora_content.save()
            print("ğŸ”„ æ›´æ–° MySQL sora_content",flush=True)

        # å»ºç«‹ SoraMediaï¼ˆä¸¤ä¸ªæœºå™¨äººæ¥æºï¼‰
        media_data = [
            {
                'source_bot_name': row.source_bot_name,
                'file_id': row.file_id,
                'thumb_file_id': row.thumb_file_id
            },
            {
                'source_bot_name': row.shell_bot_name,
                'file_id': row.shell_file_id,
                'thumb_file_id': row.shell_thumb_file_id
            }
        ]

        for media in media_data:
            existing = SoraMedia.select().where(
                (SoraMedia.content_id == sora_content.id) &
                (SoraMedia.source_bot_name == media["source_bot_name"])
            ).first()

            if existing:
                existing.file_id = media["file_id"]
                existing.thumb_file_id = media["thumb_file_id"]
                existing.save()
                print(f"  ğŸ”„ æ›´æ–° MySQL sora_media [{media['source_bot_name']}]",flush=True)
            else:
                SoraMedia.create(content_id=sora_content.id, **media)
                print(f"  âœ… æ–°å¢ MySQL sora_media [{media['source_bot_name']}]",flush=True)


        # æ›´æ–°åŸå§‹è¡¨çŠ¶æ€
        row.update_content = int(time.time())
        row.save()

        # åŒæ­¥åˆ° PostgreSQL
        if SYNC_TO_POSTGRES:
            sync_to_postgres(sora_content)
            sync_media_to_postgres(sora_content.id, media_data)
            print("ğŸš€ åŒæ­¥åˆ° PostgreSQL å®Œæˆ",flush=True)

    DB_MYSQL.close()


def sync_pending_sora_to_postgres():
    if not SYNC_TO_POSTGRES:
        print("ğŸ”’ SYNC_TO_POSTGRES ä¸º Falseï¼Œè·³è¿‡ PostgreSQL åŒæ­¥", flush=True)
        return

    print("\nğŸš€ å¼€å§‹åŒæ­¥ stage = 'pending' çš„ sora_content åˆ° PostgreSQL...", flush=True)
    from playhouse.shortcuts import model_to_dict

    ensure_connection()  # âœ… MySQL è¿æ¥/ä¿æ´»

    # âœ… å…³é”®ï¼šå¤ç”¨å·²å¼€å¯è¿æ¥ï¼›å¦‚æœå¤–éƒ¨å·² connectï¼Œè¿™é‡Œä¸ä¼šæŠ¥é”™
    DB_PG.connect(reuse_if_open=True)

    rows = SoraContent.select().where(SoraContent.stage == "pending").limit(BATCH_LIMIT)

    for row in rows:
        model_data = model_to_dict(row, recurse=False)

        for ignored in ('content_seg_tsv', 'created_at', 'updated_at'):
            model_data.pop(ignored, None)

        model_data["id"] = row.id  # MySQL/PG ä¸»é”®ä¸€è‡´

        with DB_PG.atomic():
            try:
                existing = SoraContentPg.get(SoraContentPg.id == row.id)
                for k, v in model_data.items():
                    setattr(existing, k, v)
                existing.save()
                print(f"âœ… å·²æ›´æ–° PostgreSQL sora_content.id = {row.id}", flush=True)
            except SoraContentPg.DoesNotExist:
                SoraContentPg.create(**model_data)
                print(f"âœ… å·²æ–°å¢ PostgreSQL sora_content.id = {row.id}", flush=True)

        # å›å†™ MySQLï¼šstage = updated
        row.stage = "updated"
        row.save()

    # â—ä¸è¦åœ¨è¿™é‡Œ close DB_PGï¼ˆäº¤ç»™ __main__ finally ç»Ÿä¸€å…³ï¼‰
    DB_MYSQL.close()


def sync_pending_sora_to_postgres2():
    if not SYNC_TO_POSTGRES:
        print("ğŸ”’ SYNC_TO_POSTGRES ä¸º Falseï¼Œè·³è¿‡ PostgreSQL åŒæ­¥",flush=True)
        return

    print("\nğŸš€ å¼€å§‹åŒæ­¥ stage = 'pending' çš„ sora_content åˆ° PostgreSQL...",flush=True)
    from playhouse.shortcuts import model_to_dict

    # DB_MYSQL.connect()
    ensure_connection()  # âœ… æ¨èå†™æ³•
    DB_PG.connect()

    rows = SoraContent.select().where(SoraContent.stage == "pending").limit(BATCH_LIMIT)

    for row in rows:
        # print(f"ğŸ”„ åŒæ­¥ä¸­ï¼šsource_id = {row.source_id}",flush=True)

        model_data = model_to_dict(row, recurse=False)
        # å»é™¤ä¸å¿…è¦å­—æ®µ
        for ignored in ('content_seg_tsv', 'created_at', 'updated_at'):
            model_data.pop(ignored, None)
        model_data["id"] = row.id  # å¼ºåˆ¶ä½¿ç”¨ç›¸åŒä¸»é”®

        try:
            existing = SoraContentPg.get(SoraContentPg.id == row.id)
            for k, v in model_data.items():
                setattr(existing, k, v)
            existing.save()
            print(f"âœ… å·²æ›´æ–° PostgreSQL sora_content.id = {row.id}",flush=True)
        except SoraContentPg.DoesNotExist:
            SoraContentPg.create(**model_data)
            print(f"âœ… å·²æ–°å¢ PostgreSQL sora_content.id = {row.id}",flush=True)

        # âœ… å›å†™ MySQLï¼šstage = "updated"
        row.stage = "updated"
        row.save()
        # print(f"ğŸ“ å·²æ›´æ–°ï¼šsource_id{row.source_id} =>MySQL sora_content.stage = 'updated'",flush=True)


    DB_MYSQL.close()
    DB_PG.close()

def sync_pending_product_to_postgres_old():
    from peewee import IntegrityError
    if not SYNC_TO_POSTGRES:
        print("ğŸ”’ SYNC_TO_POSTGRES ä¸º Falseï¼Œè·³è¿‡ PostgreSQL åŒæ­¥",flush=True)
        return

    print("\nğŸš€ å¼€å§‹åŒæ­¥ stage = 'pending' çš„ product åˆ° PostgreSQL...",flush=True)
    from playhouse.shortcuts import model_to_dict

    # DB_MYSQL.connect()
    ensure_connection()  # âœ… æ¨èå†™æ³•
    DB_PG.connect()

    rows = Product.select().where(Product.stage == "pending").limit(BATCH_LIMIT)

    for row in rows:
        # print(f"ğŸ”„ åŒæ­¥ä¸­ï¼šsource_id = {row.source_id}")

        model_data = model_to_dict(row, recurse=False)
        # å»é™¤ä¸å¿…è¦å­—æ®µ
        for ignored in ('stage',):
            model_data.pop(ignored, None)

        model_data["content_id"] = row.content_id  # å¼ºåˆ¶ä½¿ç”¨ç›¸åŒä¸»é”®

        from peewee import IntegrityError

        try:
            existing = ProductPg.get(ProductPg.content_id == row.content_id)
            for k, v in model_data.items():
                setattr(existing, k, v)
            existing.save()

        except ProductPg.DoesNotExist:
            try:
                ProductPg.create(**model_data)

            except IntegrityError as e:
                msg = str(e)
                # åªå¤„ç†ä½ æŒ‡å®šçš„ï¼šproduct_pkey ä¸»é”®å†²çª
                if 'duplicate key value violates unique constraint "product_pkey"' in msg:
                    conflict_id = model_data.get("id")
                    if conflict_id is None:
                        raise  # æ²¡æœ‰ id æ— æ³•æ‰§è¡Œâ€œåˆ å†æ’â€ï¼Œäº¤å›ä¸Šå±‚å¤„ç†

                    print(f"âš ï¸ product_pkey å†²çªï¼šid={conflict_id}ï¼Œå¼ºåˆ¶åˆ é™¤ PostgreSQL æ—§è®°å½•åé‡å»º", flush=True)

                    # å¼ºåˆ¶åˆ é™¤å†²çªä¸»é”®è¡Œ
                    ProductPg.delete().where(ProductPg.id == conflict_id).execute()

                    # å†æ’å…¥ä¸€æ¬¡
                    ProductPg.create(**model_data)

                else:
                    # ä¸æ˜¯ product_pkey çš„å†²çªï¼Œä¸åšç ´åæ€§æ“ä½œ
                    raise


        # âœ… å›å†™ MySQLï¼šstage = "updated"
        row.stage = "updated"
        row.save()
        # print(f"ğŸ“ å·²æ›´æ–°ï¼šcontent_id{row.content_id} =>MySQL Product.stage = 'updated'",flush=True)


    DB_MYSQL.close()
    DB_PG.close()


from peewee import IntegrityError

def sync_pending_product_to_postgres():
    if not SYNC_TO_POSTGRES:
        print("ğŸ”’ SYNC_TO_POSTGRES ä¸º Falseï¼Œè·³è¿‡ PostgreSQL åŒæ­¥", flush=True)
        return

    print("\nğŸš€ å¼€å§‹åŒæ­¥ stage = 'pending' çš„ product åˆ° PostgreSQL...", flush=True)
    from playhouse.shortcuts import model_to_dict

    ensure_connection()
    DB_PG.connect(reuse_if_open=True)

    rows = Product.select().where(Product.stage == "pending").limit(BATCH_LIMIT)

    def _is_unique_violation(e: Exception, constraint: str) -> bool:
        s = str(e)
        return ('duplicate key value violates unique constraint' in s) and (f'"{constraint}"' in s)

    def _upsert_by_id_once(model_data: dict, target_id: int):
        # å…³é”®ï¼šè®© IntegrityError ç›´æ¥æŠ›å‡º atomic()ï¼Œè§¦å‘è‡ªåŠ¨ rollback
        with DB_PG.atomic():
            existing = ProductPg.get_or_none(ProductPg.id == target_id)
            if existing is None:
                ProductPg.create(**model_data)
            else:
                for k, v in model_data.items():
                    setattr(existing, k, v)
                existing.save()

    def _delete_conflict_content_id(target_content_id: int, target_id: int):
        # ç‹¬ç«‹äº‹åŠ¡åšæ¸…ç†
        with DB_PG.atomic():
            (ProductPg
             .delete()
             .where((ProductPg.content_id == target_content_id) & (ProductPg.id != target_id))
             .execute())

    def _delete_conflict_id(target_id: int):
        with DB_PG.atomic():
            ProductPg.delete().where(ProductPg.id == target_id).execute()

    for row in rows:
        model_data = model_to_dict(row, recurse=False)
        model_data.pop("stage", None)

        target_id = model_data.get("id")
        target_content_id = model_data.get("content_id")

        if target_id is None:
            raise ValueError("Product row has no id; cannot sync by id")

        try:
            # 1) id ä¼˜å…ˆï¼šå…ˆ upsert
            _upsert_by_id_once(model_data, target_id)

        except IntegrityError as e:
            # 2) content_id å†²çªï¼šåˆ æ‰å ç”¨è¯¥ content_id çš„å…¶ä»–è¡Œï¼Œå†é‡è¯•
            if _is_unique_violation(e, "uq_product_content_id"):
                print(
                    f"âš ï¸ uq_product_content_id å†²çªï¼šcontent_id={target_content_id}ï¼Œåˆ é™¤é‡å¤ content_id åé‡è¯•",
                    flush=True
                )
                _delete_conflict_content_id(target_content_id, target_id)
                _upsert_by_id_once(model_data, target_id)

            # 3) ä¸»é”®å†²çªï¼šåˆ åŒ id çš„æ—§è¡Œï¼Œå†é‡è¯•ï¼›è‹¥ä»æ’ content_idï¼Œå†åˆ  content_id å ç”¨è€…å†è¯•ä¸€æ¬¡
            elif _is_unique_violation(e, "product_pkey"):
                print(f"âš ï¸ product_pkey å†²çªï¼šid={target_id}ï¼Œåˆ é™¤åŒ id åé‡è¯•", flush=True)
                _delete_conflict_id(target_id)

                try:
                    _upsert_by_id_once(model_data, target_id)
                except IntegrityError as e2:
                    if _is_unique_violation(e2, "uq_product_content_id"):
                        print(
                            f"âš ï¸ uq_product_content_id å†²çªï¼ˆåœ¨åˆ é™¤ id åä»å‘ç”Ÿï¼‰ï¼šcontent_id={target_content_id}ï¼Œåˆ é™¤é‡å¤ content_id åå†é‡è¯•",
                            flush=True
                        )
                        _delete_conflict_content_id(target_content_id, target_id)
                        _upsert_by_id_once(model_data, target_id)
                    else:
                        raise
            else:
                raise

        # âœ… å›å†™ MySQLï¼šstage = "updated"
        row.stage = "updated"
        row.save()

        print(f"âœ… å·²åŒæ­¥ PostgreSQL product.id={target_id}, content_id={target_content_id}", flush=True)

    # ä¸è¦åœ¨è¿™é‡Œ close PGï¼ˆå»ºè®®ç»Ÿä¸€åœ¨ __main__ finally å…³ï¼‰
    # DB_MYSQL.close() / DB_PG.close() äº¤ç»™ä¸»æµç¨‹ç»Ÿä¸€ç®¡ç†



def sync_pending_product_to_postgres2():
    from peewee import IntegrityError

    if not SYNC_TO_POSTGRES:
        print("ğŸ”’ SYNC_TO_POSTGRES ä¸º Falseï¼Œè·³è¿‡ PostgreSQL åŒæ­¥", flush=True)
        return

    print("\nğŸš€ å¼€å§‹åŒæ­¥ stage = 'pending' çš„ product åˆ° PostgreSQL...", flush=True)
    from playhouse.shortcuts import model_to_dict

    ensure_connection()  # âœ… æ¨èå†™æ³•
    DB_PG.connect()

    rows = Product.select().where(Product.stage == "pending").limit(BATCH_LIMIT)

    def _is_unique_violation(e: Exception, constraint: str) -> bool:
        s = str(e)
        return ('duplicate key value violates unique constraint' in s) and (f'"{constraint}"' in s)

    for row in rows:
        model_data = model_to_dict(row, recurse=False)
        model_data.pop("stage", None)

        target_id = model_data.get("id")
        target_content_id = model_data.get("content_id")

        if target_id is None:
            raise ValueError("Product row has no id; cannot sync by id")

        with DB_PG.atomic():
            # 1) ä»¥ id ä¸ºä¸»ï¼šå…ˆæ‰¾åŒ id çš„ existing
            existing = ProductPg.get_or_none(ProductPg.id == target_id)

            try:
                if existing is None:
                    # INSERT
                    ProductPg.create(**model_data)
                else:
                    # UPDATEï¼ˆæŒ‰ id æ›´æ–°è¿™ä¸€æ¡ï¼‰
                    for k, v in model_data.items():
                        setattr(existing, k, v)
                    existing.save()

            except IntegrityError as e:
                # 2) è‹¥ content_id å”¯ä¸€å†²çªï¼šåˆ é™¤â€œå ç”¨è¯¥ content_id çš„å…¶ä»–è¡Œâ€ï¼Œå†é‡è¯•ä¸€æ¬¡
                if _is_unique_violation(e, "uq_product_content_id"):
                    print(
                        f"âš ï¸ uq_product_content_id å†²çªï¼šcontent_id={target_content_id}ï¼Œåˆ é™¤é‡å¤ content_id åé‡è¯•",
                        flush=True
                    )

                    # åªåˆ é™¤â€œåŒ content_id ä¸” id != å½“å‰ idâ€çš„è¡Œï¼Œé¿å…è¯¯åˆ å½“å‰ç›®æ ‡
                    (ProductPg
                     .delete()
                     .where((ProductPg.content_id == target_content_id) & (ProductPg.id != target_id))
                     .execute())

                    # é‡è¯•ï¼ˆåŒä¸€äº‹åŠ¡å†…ï¼‰
                    existing2 = ProductPg.get_or_none(ProductPg.id == target_id)
                    if existing2 is None:
                        ProductPg.create(**model_data)
                    else:
                        for k, v in model_data.items():
                            setattr(existing2, k, v)
                        existing2.save()

                # 3) è‹¥ä¸»é”®å†²çªï¼ˆæå°‘æ•°ï¼šå¹¶å‘/å†å²å¼‚å¸¸ï¼‰ï¼šåˆ åŒ id å†æ’ä¸€æ¬¡
                elif _is_unique_violation(e, "product_pkey"):
                    print(
                        f"âš ï¸ product_pkey å†²çªï¼šid={target_id}ï¼Œåˆ é™¤åŒ id åé‡è¯•",
                        flush=True
                    )
                    ProductPg.delete().where(ProductPg.id == target_id).execute()

                    # è¿™é‡Œå†æ¬¡æ’å…¥ä»å¯èƒ½å›  content_id å†²çªå¤±è´¥ï¼Œè®©ä¸Šå±‚çœ‹åˆ°_


if __name__ == "__main__":
    ensure_connection()

    if SYNC_TO_POSTGRES:
        # åªè¿ä¸€æ¬¡
        DB_PG.connect(reuse_if_open=True)

    try:
        process_documents()            # é‡Œé¢ä¸è¦å† connect/close PG
        process_videos()               # åŒä¸Š
        # process_scrap()
        sync_pending_sora_to_postgres()  # åŒä¸Š
        sync_pending_product_to_postgres()
    finally:
        # ç»Ÿä¸€å…³é—­
        if not DB_MYSQL.is_closed():
            DB_MYSQL.close()
        if SYNC_TO_POSTGRES and (not DB_PG.is_closed()):
            DB_PG.close()

